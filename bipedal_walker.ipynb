{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from gym import wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = 'BipedalWalker-v2'   # Name of OpenAI gym environment\n",
    "REWARD_HISTORY = []             # History of rewards used for plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hp:\n",
    "    # Hyperparameters\n",
    "    \n",
    "    def __init__(self,\n",
    "                 num_steps=1500,\n",
    "                 episode_length=3000,\n",
    "                 num_deltas=20,\n",
    "                 num_best_deltas=20,\n",
    "                 record_every=100,\n",
    "                 env_name='BipedalWalker-v2',\n",
    "                 seed=42,\n",
    "                 noise=0.03,\n",
    "                 alpha=0.02):\n",
    "        self.num_steps = num_steps\n",
    "        self.episode_length = episode_length\n",
    "        self.num_deltas = num_deltas\n",
    "        self.num_best_deltas = num_best_deltas\n",
    "        assert self.num_best_deltas <= self.num_deltas\n",
    "        self.record_every = record_every\n",
    "        self.env_name = env_name\n",
    "        self.seed = seed\n",
    "        self.noise = noise\n",
    "        self.alpha = alpha      # Learning rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalizer:\n",
    "    # Normalizes input values using standard normalization algorithm\n",
    "    \n",
    "    def __init__(self, num_inputs):\n",
    "        # Creates empty arrays of the size of the input space\n",
    "        self.n = np.zeros(num_inputs)\n",
    "        self.mean = np.zeros(num_inputs)\n",
    "        self.mean_diff = np.zeros(num_inputs)\n",
    "        self.variance = np.zeros(num_inputs)\n",
    "\n",
    "    def observe(self, x):\n",
    "        # Computes running average and variance of the input values\n",
    "        self.n += 1.0\n",
    "        last_mean = self.mean.copy()\n",
    "        self.mean += (x - self.mean) / self.n\n",
    "        self.mean_diff += (x - last_mean) * (x - self.mean)\n",
    "        self.variance = (self.mean_diff / self.n).clip(min=1e-2)\n",
    "\n",
    "    def normalize(self, inputs):\n",
    "        obs_mean = self.mean                    # Observation mean\n",
    "        obs_std = np.sqrt(self.variance)        # Observation standard deviation\n",
    "        return (inputs - obs_mean) / obs_std\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy:\n",
    "    # Generates random noise, turns input into actions and updates the policy\n",
    "    \n",
    "    def __init__(self, input_size, output_size, hp):\n",
    "        self.theta = np.zeros((output_size, input_size))    # Initialize weight matrix with zeros\n",
    "        self.hp = hp                                        # Hyperparameters\n",
    "\n",
    "    def evaluate(self, input, delta=None, direction=None):\n",
    "        # Turns input into actions\n",
    "        if direction is None:\n",
    "            return self.theta.dot(input)\n",
    "        elif direction == '+':\n",
    "            return (self.theta + self.hp.noise * delta).dot(input)\n",
    "        elif direction == '-':\n",
    "            return (self.theta - self.hp.noise * delta).dot(input)\n",
    "\n",
    "    def sample_deltas(self):\n",
    "        # Generates random noise\n",
    "        return [np.random.randn(*self.theta.shape) for _ in range(self.hp.num_deltas)]\n",
    "\n",
    "    def update(self, rollouts, sigma_rewards):\n",
    "        # Updates the policy\n",
    "        step = np.zeros(self.theta.shape)\n",
    "        for r_pos, r_neg, delta in rollouts:\n",
    "            step += (r_pos - r_neg) * delta\n",
    "        self.theta += self.hp.alpha / (self.hp.num_best_deltas * sigma_rewards) * step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArsAgent:\n",
    "    def __init__(self,\n",
    "                 hp=None,\n",
    "                 input_size=None,\n",
    "                 output_size=None,\n",
    "                 normalizer=None,\n",
    "                 policy=None,\n",
    "                 monitor_dir=None):\n",
    "        self.hp = hp or Hp\n",
    "        np.random.seed(self.hp.seed)\n",
    "        self.env = gym.make(self.hp.env_name)\n",
    "        if monitor_dir is not None:\n",
    "            should_record = lambda i: self.record_video\n",
    "            self.env = wrappers.Monitor(self.env, monitor_dir, video_callable=should_record, force=True)\n",
    "        self.hp.episode_length = self.env.spec.timestep_limit or self.hp.episode_length\n",
    "        self.input_size = input_size or self.env.observation_space.shape[0]\n",
    "        self.output_size = output_size or self.env.action_space.shape[0]\n",
    "        self.normalizer = normalizer or Normalizer(self.input_size)\n",
    "        self.policy = policy or Policy(self.input_size, self.output_size, self.hp)\n",
    "        self.record_video = False\n",
    "\n",
    "    def explore(self, direction=None, delta=None):\n",
    "        # Explores the policy and returns the sum of the rewards accumulated\n",
    "        state = self.env.reset()\n",
    "        done = False\n",
    "        num_plays = 0.0\n",
    "        sum_rewards = 0.0\n",
    "        while not done and num_plays < self.hp.episode_length:\n",
    "            self.normalizer.observe(state)\n",
    "            state = self.normalizer.normalize(state)\n",
    "            action = self.policy.evaluate(state, delta, direction)\n",
    "            state, reward, done, _ = self.env.step(action)\n",
    "            reward = max(min(reward, 1), -1)\n",
    "            sum_rewards += reward\n",
    "            num_plays += 1\n",
    "        return sum_rewards\n",
    "\n",
    "    def train(self):\n",
    "        # Trains the agent\n",
    "        for step in range(self.hp.num_steps):\n",
    "            deltas = self.policy.sample_deltas()\n",
    "            pos_rewards = [0] * self.hp.num_deltas\n",
    "            neg_rewards = [0] * self.hp.num_deltas\n",
    "            \n",
    "            for k in range(self.hp.num_deltas):\n",
    "                pos_rewards[k] = self.explore(direction='+', delta=deltas[k])\n",
    "                neg_rewards[k] = self.explore(direction='-', delta=deltas[k])\n",
    "\n",
    "            sigma_rewards = np.array(pos_rewards + neg_rewards).std()\n",
    "\n",
    "            scores = {k:max(r_pos, r_neg) for k,(r_pos, r_neg) in enumerate(zip(pos_rewards, neg_rewards))}\n",
    "            order = sorted(scores.keys(), key=lambda x:scores[x], reverse=True)[:self.hp.num_best_deltas]\n",
    "            rollouts = [(pos_rewards[k], neg_rewards[k], deltas[k]) for k in order]\n",
    "\n",
    "            self.policy.update(rollouts, sigma_rewards)\n",
    "\n",
    "            if step % self.hp.record_every == 0:\n",
    "                self.record_video = True\n",
    "\n",
    "            reward_evaluation = self.explore()\n",
    "            REWARD_HISTORY.append(reward_evaluation)\n",
    "            print('Step: {} | Reward: {}'.format(step, reward_evaluation))\n",
    "            self.record_video = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir(base, name):\n",
    "    path = os.path.join(base, name)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    return path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_dir = mkdir('.', 'videos')\n",
    "monitor_dir = mkdir(videos_dir, ENV_NAME)\n",
    "hp = Hp(env_name=ENV_NAME)\n",
    "agent = ArsAgent(hp=hp, monitor_dir=monitor_dir)\n",
    "agent.train()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(REWARD_HISTORY)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Rewards over Time')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![plot](https://image.ibb.co/igf4GV/plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result after 800 steps\n",
    "\n",
    "![demo](https://image.ibb.co/fPkDbV/bipedal.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
